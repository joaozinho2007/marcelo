{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUWO5QkC_g-4"
      },
      "source": [
        "O **Tiro com Arco** é um jogo em que os jogadores atiram flechas de ponta afiada em um alvo redondo com 10 anéis.\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/4de9132a-c71d-42ce-9099-3293e8805fd9.jpg\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QtHLAqv3wP3"
      },
      "source": [
        "## Problema de Aprendizado por Reforço (RL) a Resolver\n",
        "Acerte o centro do alvo que proporciona a recompensa máxima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osb6FQ74YZtE"
      },
      "source": [
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/40656a8c-14e2-4dd7-9f9e-4c17669b9182.png\" width=300>\n",
        "\n",
        "\n",
        "Número de **Estados**: ?\n",
        "\n",
        "Número de **Ações**: ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2oIipDmeqap"
      },
      "outputs": [],
      "source": [
        "#Importe as bibliotecas\n",
        "#importe as bibliotecas\n",
        "import numpy as np\n",
        "\n",
        "# Numero de estados e ações\n",
        "num_estados = 5\n",
        "num_acoes = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujmi3BO54LfJ"
      },
      "source": [
        "## Matriz de Recompensas\n",
        "A Matriz de Recompensas representa os estados como linhas e as ações como colunas com os respectivos valores de recompensas atribuídos a um determinado par de estado e ação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUqPgOl0eh2u"
      },
      "outputs": [],
      "source": [
        "#Crie a matriz de recompensas\n",
        "#Defina a matriz de recompensas\n",
        "reward_matrix = np.array([[0, -10, -10],  # Estado 0\n",
        "    [-10, 10, -10],  # Estado 1\n",
        "    [-10, -10, 10],  # Estado 2\n",
        "    [-10, 10, -10],  # Estado 3\n",
        "    [-10, -10, 0]    # Estado 4\n",
        "    ])\n",
        "\n",
        "# Matriz Q iniclamente todos os elementos zerados\n",
        "Q_matrix = np.zeros((num_estados, num_acoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af-CAmdfkDQY"
      },
      "source": [
        "## Execute Ações Aleatoriamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibSLCyMyigmK"
      },
      "outputs": [],
      "source": [
        "#Defina shoot()\n",
        "#Defina shoot()\n",
        "def shoot():\n",
        "    return np.random.randint(num_acoes)\n",
        "\n",
        "#Executar a ação\n",
        "def take_action(state, reward_matrix):\n",
        "    action = shoot()\n",
        "    reward = reward_matrix[state, action]\n",
        "    print(f\"Ação escolhida: {action}, Recompensa: {reward}\")\n",
        "\n",
        "    #Testando a função take action\n",
        "    estado_atual = 2\n",
        "    acao, recompensa = take_action(estado_atual, reward_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXKyVT28hHoH"
      },
      "source": [
        "##Matriz Q\n",
        "O **Aprendizado Q** é um algoritmo de aprendizado por reforço. Dado o estado atual, ele ajuda a encontrar a melhor ação a ser tomada pelo agente.\n",
        "\n",
        "A **Matriz Q** representa a recompensa recebida após uma ação específica no estado atual. Inicialmente, todos os elementos da matriz Q estão zerados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNYwOV7ogtw1"
      },
      "outputs": [],
      "source": [
        "#Crie a matriz Q\n",
        "#Crie a matriz Q\n",
        "print(\"\\nMatriz Q inicial:\")\n",
        "print(Q_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c95A4SOkGdN"
      },
      "source": [
        "##Execute uma Ação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSBm-8CJ0UfK"
      },
      "outputs": [],
      "source": [
        "#Defina take_action\n",
        "\n",
        "def take_action(reward_matrix):\n",
        "  \n",
        "     #Chame a função shoot() para obter a ação\n",
        "     action = shoot()\n",
        "     #Imprima a ação\n",
        "     print(f\"Ação escolhida: {action}\")\n",
        "     #Obtenha a recompensa correspondente usando a matriz de recompensas\n",
        "     reward = reward_matrix[num_estados, action]\n",
        "     #Imprima a recompensa\n",
        "     print(f\"Recompensa obtida: {reward}\")\n",
        "\n",
        "     return action, reward\n",
        "\n",
        "#Teste a função take_action\n",
        "estado_atual = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKy1VkgO4ZhP"
      },
      "source": [
        "## Atualize a Matriz Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_U6NFICkhGMF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "num_estados = 0\n",
        "num_acoes = 0\n",
        "\n",
        "\n",
        "# Suponha que a matriz Q seja global (fora da função) para que ela seja compartilhada e atualizada entre episódios\n",
        "Q_matrix = np.zeros((num_estados, num_acoes))  # Certifique-se de que num_estados e num_acoes estão definidos\n",
        "\n",
        "# Defina o método run_episode()\n",
        "def run_episode(reward_matrix, shoot_per_game=5):\n",
        "    global Q_matrix  # Utilizando a matriz Q global\n",
        "\n",
        "    score = 0\n",
        "\n",
        "    # Use o loop for para percorrer o número de tentativas\n",
        "    for tentativa in range(shoot_per_game):\n",
        "        # Imprima o número do tiro\n",
        "        print(f\"Tentativa {tentativa + 1}:\")\n",
        "\n",
        "        # Chame o método take_action para obter a ação e a recompensa\n",
        "        estado_atual = np.random.randint(num_estados)  # Supondo que os estados são números inteiros aleatórios\n",
        "        acao, recompensa = take_action(estado_atual, reward_matrix)\n",
        "\n",
        "        # Aumente a pontuação\n",
        "        score += recompensa\n",
        "\n",
        "        # Imprima o número do tiro final\n",
        "        print(f\"Tentativa {tentativa + 1} - Ação: {acao}, Recompensa: {recompensa}\")\n",
        "\n",
        "    # Atualize a matriz Q\n",
        "    learning_rate = 0.1  # Taxa de aprendizado (ajuste conforme necessário)\n",
        "    discount_factor = 0.9  # Fator de desconto (ajuste conforme necessário)\n",
        "\n",
        "    for tentativa in range(shoot_per_game):\n",
        "        # Atualize a matriz Q usando a fórmula Q-learning\n",
        "        Q_matrix[estado_atual, acao] += learning_rate * (recompensa + discount_factor * np.max(Q_matrix) - Q_matrix[estado_atual, acao])\n",
        "\n",
        "    # Retorne a matriz Q atualizada\n",
        "    return Q_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5pcUO-5pJ-h"
      },
      "outputs": [],
      "source": [
        "#Chame o método run_episode para verificar a matriz Q final de um episódio\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dVY734TlZBx"
      },
      "source": [
        "## Treinar\n",
        "\n",
        "Crie uma função que execute um número de jogos, execute cada jogo por um determinado número de vezes e calcule as recompensas médias para cada vez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smfOXGYZlp7b"
      },
      "outputs": [],
      "source": [
        "# Defina a função train()\n",
        "def train(episodes):\n",
        "    # Use o loop for para percorrer os episódios\n",
        "    for episode in range(1, episodes+1):\n",
        "        # Inicialize a variável total_reward\n",
        "        total_reward = 0\n",
        "\n",
        "        # Imprima \"Episódio Inicia\" com o número do episódio\n",
        "        print(f\"Episódio Inicia {episode}\")\n",
        "\n",
        "        # Chame o método run_episode() para obter a matriz Q de um episódio\n",
        "        q_matrix = run_episode()\n",
        "\n",
        "        # A recompensa do episódio será a soma de todas as recompensas de um episódio\n",
        "        episode_reward = np.sum(q_matrix)\n",
        "\n",
        "        # Imprima a recompensa do episódio\n",
        "        print(f\"Recompensa do Episódio {episode}: {episode_reward}\")\n",
        "\n",
        "        # A recompensa total será a soma de todas as recompensas do episódio\n",
        "        total_reward += episode_reward\n",
        "\n",
        "    # Retorne total_reward\n",
        "    return total_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WYH_ioykcBC"
      },
      "source": [
        "##Treine 1000 episódios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cbjnp3PSovsE"
      },
      "outputs": [],
      "source": [
        "# Defina a função run_episode()\n",
        "def run_episode():\n",
        "    # Implemente a lógica para executar um episódio e atualizar a matriz Q\n",
        "    # Esta função dependerá dos detalhes do seu ambiente e do método de aprendizado por reforço que você está usando\n",
        "    # Certifique-se de incluir a lógica de atualização da matriz Q aqui\n",
        "    # Retorne a matriz Q no final\n",
        "\n",
        "# Defina a função train()\n",
        " train=0\n",
        " def train(episodes):\n",
        "    total_rewards = []  # Lista para armazenar as recompensas de cada episódio\n",
        "\n",
        "    # Use o loop for para percorrer os episódios\n",
        "    for episode in range(1, episodes + 1):\n",
        "        # Inicialize a variável total_reward\n",
        "        total_reward = 0\n",
        "\n",
        "        # Imprima \"Episódio Inicia\" com o número do episódio\n",
        "        print(f\"Episódio Inicia {episode}\")\n",
        "\n",
        "        # Chame o método run_episode() para obter a matriz Q de um episódio\n",
        "        q_matrix = run_episode()\n",
        "\n",
        "        # A recompensa do episódio será a soma de todas as recompensas de um episódio\n",
        "        episode_reward = np.sum(q_matrix)\n",
        "\n",
        "        # Imprima a recompensa do episódio\n",
        "        print(f\"Recompensa do Episódio {episode}: {episode_reward}\")\n",
        "\n",
        "        # A recompensa total será a soma de todas as recompensas do episódio\n",
        "        total_reward += episode_reward\n",
        "\n",
        "        # Adicione a recompensa do episódio à lista total_rewards\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "    # Calcule a recompensa média\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "\n",
        "    # Imprima a recompensa média\n",
        "    print(f\"Recompensa Média para {episodes} episódios: {avg_reward}\")\n",
        "\n",
        "    # Retorne total_rewards\n",
        "    return total_rewards\n",
        "    \n",
        "# Execute a função train() para 1000 episódios\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jBnwogyuJP0"
      },
      "source": [
        "## Conclusão: \n",
        "\n",
        "Isso nos dá uma boa ideia sobre o desempenho geral do aprendizado de reforço mais simples com o problema **um estado - múltiplas ações**, também conhecido como problema \"**K-Armed Bandit**\".\n",
        "\n",
        "Um dos principais casos de uso desse tipo de problema pode ser visto na seleção do anúncio certo entre muitos a serem exibidos na página web. A máquina pode ser ensinada a escolher o melhor anúncio com mais cliques do usuário!!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PRO-C126-Project-Boilerplate-Code(pt).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
